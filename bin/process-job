#!/usr/bin/env python3
"""Reeder Job Processor

Processes TTS jobs from the inbox directory:
1. Picks the oldest job file
2. Moves it to processing/
3. Extracts text (for URL jobs) or uses provided text
4. Generates audio with pocket-tts
5. Converts to final format (opus/mp3)
6. Updates RSS feed
7. Moves completed job to done/
"""

# Auto-bootstrap: re-exec with uv if dependencies missing
def _bootstrap():
    try:
        import trafilatura  # noqa: F401
    except ImportError:
        import os, sys
        # Re-exec with uv run using /usr/lib/reeder as project
        # UV_PROJECT_ENVIRONMENT should be set by systemd to /var/lib/reeder/.venv
        # Use --frozen to prevent writing to uv.lock in read-only /usr/lib
        project_root = "/usr/lib/reeder" if os.path.exists("/usr/lib/reeder/pyproject.toml") else os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        frozen = ["--frozen"] if project_root == "/usr/lib/reeder" else []
        os.execvp("uv", ["uv", "run", "--project", project_root] + frozen + [sys.argv[0]] + sys.argv[1:])
_bootstrap()

import json
import logging
import os
import shutil
import statistics
import subprocess
import sys
import tempfile
import tomllib
from datetime import datetime, timezone
from pathlib import Path
from urllib.parse import urlparse

# Suppress noisy transformers warnings about pad_token_id
logging.getLogger("transformers.generation.utils").setLevel(logging.ERROR)

# Status file for remote monitoring
STATUS_FILE = None


def load_config(config_path: Path | None = None) -> dict:
    """Load configuration from config.toml."""
    if config_path is None:
        # Look for config in standard locations
        candidates = [
            Path(__file__).parent.parent / "config.toml",
            Path("/etc/reeder/config.toml"),
            Path.home() / ".config/reeder/config.toml",
        ]
        for candidate in candidates:
            if candidate.exists():
                config_path = candidate
                break
        else:
            raise FileNotFoundError("No config.toml found")

    with open(config_path, "rb") as f:
        return tomllib.load(f)


def get_paths(config: dict) -> dict[str, Path]:
    """Resolve all paths from config."""
    base = Path(config["paths"]["base_dir"])
    return {
        "base": base,
        "inbox": base / config["paths"]["inbox"],
        "processing": base / config["paths"]["processing"],
        "done": base / config["paths"]["done"],
        "audio": base / config["paths"]["audio"],
        "www": base / config["paths"]["www"],
        "voices": base / config["paths"]["voices"],
        "status": base / config["paths"]["status_file"],
    }


def write_status(paths: dict, message: str):
    """Write status message to status file."""
    status_path = paths["status"]
    status_path.parent.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now(timezone.utc).isoformat()
    with open(status_path, "w") as f:
        f.write(f"{timestamp}\n{message}\n")
    print(f"[STATUS] {message}", flush=True)


def get_oldest_job(inbox: Path) -> Path | None:
    """Get the oldest job file from inbox (by filename, which includes timestamp)."""
    jobs = sorted(inbox.glob("*.json"))
    return jobs[0] if jobs else None


def extract_text_trafilatura(url: str) -> str:
    """Extract article text using trafilatura."""
    import trafilatura

    downloaded = trafilatura.fetch_url(url)
    if not downloaded:
        raise ValueError(f"Failed to fetch URL: {url}")

    text = trafilatura.extract(
        downloaded,
        include_comments=False,
        include_tables=False,
    )
    if not text:
        raise ValueError(f"Failed to extract text from: {url}")

    return text


def extract_text_pup(url: str, selector: str) -> str:
    """Extract text using curl + pup with a CSS selector."""
    result = subprocess.run(
        ["curl", "-sL", url],
        capture_output=True,
        text=True,
        check=True,
    )
    html = result.stdout

    result = subprocess.run(
        ["pup", selector],
        input=html,
        capture_output=True,
        text=True,
        check=True,
    )
    text = result.stdout.strip()

    if not text:
        raise ValueError(f"No text found with selector '{selector}' at {url}")

    return text


def extract_text_readability(url: str) -> str:
    """Extract text using readability-cli."""
    result = subprocess.run(
        ["readable", url, "-p"],
        capture_output=True,
        text=True,
        check=True,
    )
    text = result.stdout.strip()

    if not text:
        raise ValueError(f"Failed to extract text with readability from: {url}")

    return text


def extract_text(job: dict, config: dict) -> str:
    """Extract text based on job type and extractor settings."""
    if job["type"] == "text":
        return job["text"]

    url = job["url"]
    extractor = job.get("extractor", config["extractors"]["default"])

    # Check for site-specific extractor
    if extractor == "auto":
        domain = urlparse(url).netloc
        site_extractors = config.get("extractors", {}).get("sites", {})
        if domain in site_extractors:
            extractor = {"pup": site_extractors[domain]}
        else:
            extractor = "trafilatura"

    if isinstance(extractor, dict) and "pup" in extractor:
        return extract_text_pup(url, extractor["pup"])
    elif extractor == "readability":
        return extract_text_readability(url)
    else:  # trafilatura (default)
        return extract_text_trafilatura(url)


# Global TTS model (lazy-loaded to avoid startup cost when no jobs)
_tts_model = None
TTS_MODEL_NAME = "Qwen/Qwen3-TTS-12Hz-0.6B-Base"


def get_tts_model(device: str = "cpu"):
    """Get or load the Qwen3-TTS model."""
    global _tts_model
    if _tts_model is None:
        import torch
        from qwen_tts import Qwen3TTSModel
        
        print(f"Loading TTS model: {TTS_MODEL_NAME}", flush=True)
        print(f"  Device: {device}", flush=True)
        print(f"  Dtype: torch.float32", flush=True)
        _tts_model = Qwen3TTSModel.from_pretrained(
            TTS_MODEL_NAME,
            device_map=device,
            dtype=torch.float32,
        )
        print(f"  Model loaded successfully.", flush=True)
    return _tts_model


def split_text_into_chunks(text: str, tokenizer, max_tokens: int = 50) -> list[str]:
    """Split text into chunks at sentence boundaries using the Qwen tokenizer.
    
    Uses actual token counts for accurate chunking. Avoids splitting on
    common abbreviations (Mr., Dr., etc.).
    
    Args:
        text: The text to split
        tokenizer: The Qwen tokenizer instance
        max_tokens: Maximum tokens per chunk (default 50)
    
    Returns:
        List of text chunks, each ending at a sentence boundary when possible
    """
    import re
    
    # Normalize whitespace
    text = text.strip()
    text = re.sub(r'\s+', ' ', text)
    
    # Check if text is short enough already
    total_tokens = len(tokenizer(text, return_tensors='pt').input_ids[0])
    if total_tokens <= max_tokens:
        return [text]
    
    # Common abbreviations that shouldn't trigger sentence splits
    # We protect them by temporarily replacing them
    abbreviations = [
        (r'\bMr\.', 'Mr\x00'),
        (r'\bMrs\.', 'Mrs\x00'),
        (r'\bMs\.', 'Ms\x00'),
        (r'\bDr\.', 'Dr\x00'),
        (r'\bProf\.', 'Prof\x00'),
        (r'\bSr\.', 'Sr\x00'),
        (r'\bJr\.', 'Jr\x00'),
        (r'\bSt\.', 'St\x00'),
        (r'\bvs\.', 'vs\x00'),
        (r'\betc\.', 'etc\x00'),
        (r'\be\.g\.', 'eg\x00'),
        (r'\bi\.e\.', 'ie\x00'),
        (r'\bU\.S\.', 'US\x00'),
        (r'\bU\.K\.', 'UK\x00'),
        (r'\bNo\.', 'No\x00'),
        (r'\bCo\.', 'Co\x00'),
        (r'\bInc\.', 'Inc\x00'),
        (r'\bLtd\.', 'Ltd\x00'),
        (r'\bCorp\.', 'Corp\x00'),
    ]
    
    protected = text
    for pattern, replacement in abbreviations:
        protected = re.sub(pattern, replacement, protected)
    
    # Split into sentences at .!? followed by space and capital letter or end
    # This pattern requires the next char to be uppercase or end of string
    sentences_raw = re.split(r'(?<=[.!?])\s+(?=[A-Z]|$)', protected)
    
    # Restore abbreviations
    sentences = []
    for s in sentences_raw:
        restored = s.replace('\x00', '.')
        if restored.strip():
            sentences.append(restored.strip())
    
    if not sentences:
        sentences = [text]
    
    # Count tokens for each sentence
    sentence_tokens = []
    for sentence in sentences:
        num_tokens = len(tokenizer(sentence, return_tensors='pt').input_ids[0])
        sentence_tokens.append((sentence, num_tokens))
    
    # Group sentences into chunks
    chunks = []
    current_chunk = ""
    current_tokens = 0
    
    for sentence, num_tokens in sentence_tokens:
        if not current_chunk:
            current_chunk = sentence
            current_tokens = num_tokens
        elif current_tokens + num_tokens <= max_tokens:
            current_chunk += " " + sentence
            current_tokens += num_tokens
        else:
            chunks.append(current_chunk)
            current_chunk = sentence
            current_tokens = num_tokens
    
    if current_chunk:
        chunks.append(current_chunk)
    
    # Handle chunks that are still too long (single long sentence)
    final_chunks = []
    for chunk in chunks:
        chunk_tokens = len(tokenizer(chunk, return_tensors='pt').input_ids[0])
        if chunk_tokens <= max_tokens:
            final_chunks.append(chunk)
        else:
            # Split on clause boundaries (comma, semicolon, colon)
            parts = re.split(r'(?<=[,;:])\s+', chunk)
            sub_chunk = ""
            sub_tokens = 0
            for part in parts:
                part_tokens = len(tokenizer(part, return_tensors='pt').input_ids[0])
                if not sub_chunk:
                    sub_chunk = part
                    sub_tokens = part_tokens
                elif sub_tokens + part_tokens <= max_tokens:
                    sub_chunk += " " + part
                    sub_tokens += part_tokens
                else:
                    final_chunks.append(sub_chunk)
                    sub_chunk = part
                    sub_tokens = part_tokens
            if sub_chunk:
                final_chunks.append(sub_chunk)
    
    return final_chunks


def generate_audio(text: str, output_path: Path, job: dict, config: dict, paths: dict):
    """Generate audio using Qwen3-TTS with chunking for long texts.
    
    Streams each chunk's audio to a raw PCM temp file to avoid holding
    hours of audio in memory. The PCM file is converted to WAV at the end.
    """
    import numpy as np
    import soundfile as sf
    
    voice = job.get("voice", "default")
    
    # Resolve voice to .wav file
    if voice == "default":
        default_voice = config["tts"]["default_voice"]
        # Strip extension and add .wav
        voice_name = Path(default_voice).stem
        voice_wav = paths["voices"] / f"{voice_name}.wav"
    elif not Path(voice).is_absolute():
        voice_name = Path(voice).stem
        voice_wav = paths["voices"] / f"{voice_name}.wav"
    else:
        voice_wav = Path(voice)
        voice_name = voice_wav.stem
    
    # Find transcript file (.txt next to .wav)
    voice_txt = voice_wav.with_suffix(".txt")
    if not voice_txt.exists():
        raise ValueError(f"Voice transcript not found: {voice_txt}")
    if not voice_wav.exists():
        raise ValueError(f"Voice audio not found: {voice_wav}")
    
    ref_text = voice_txt.read_text().strip()
    
    device = config["tts"].get("device", "cpu")
    temperature = job.get("temperature", config["tts"].get("temperature", 0.9))
    language = job.get("language", "Auto")
    
    print(f"  Voice: {voice_name}", flush=True)
    print(f"  Reference audio: {voice_wav}", flush=True)
    print(f"  Temperature: {temperature}", flush=True)
    print(f"  Language: {language}", flush=True)
    
    # Load model
    tts = get_tts_model(device)
    
    # Split text into chunks for better generation quality
    # Use the model's tokenizer for accurate token counting
    tokenizer = tts.processor.tokenizer
    max_tokens = config["tts"].get("max_tokens_per_chunk", 100)
    chunks = split_text_into_chunks(text, tokenizer, max_tokens=max_tokens)
    total_chunks = len(chunks)
    job_name = job.get("title", "untitled")
    print(f"  Text split into {total_chunks} chunk(s)", flush=True)
    
    # Create voice clone prompt once and reuse for all chunks
    print(f"  Creating voice clone prompt...", flush=True)
    voice_clone_prompt = tts.create_voice_clone_prompt(
        ref_audio=str(voice_wav),
        ref_text=ref_text,
        x_vector_only_mode=False,
    )
    
    # Generate audio for each chunk
    # Note: pad_token_id set explicitly to suppress warning messages
    gen_kwargs = dict(
        pad_token_id=tts.processor.tokenizer.eos_token_id,
        max_new_tokens=4096,
        do_sample=True,
        top_k=50,
        top_p=1.0,
        temperature=temperature,
        repetition_penalty=1.05,
        subtalker_dosample=True,
        subtalker_top_k=50,
        subtalker_top_p=1.0,
        subtalker_temperature=temperature,
    )
    
    # Stream chunks to a raw PCM temp file to avoid holding all audio in memory
    pcm_path = output_path.with_suffix(".pcm")
    sample_rate = None
    # preseed history based on past runs
    sample_per_token_history = [5110,4793,5234,5889,5130,5941,5877,5138,5607,6370,6260,6381,5894,5538,6027,5280]
    std_dev_limit = 2.2
    
    try:
        with open(pcm_path, "wb") as pcm_file:
            for i, chunk in enumerate(chunks):
                chunk_tokens = len(tokenizer(chunk, return_tensors="pt").input_ids[0])
                write_status(paths, f"Generating audio: {job_name} (chunk {i+1:>3}/{total_chunks},{len(chunk):>3}c,{chunk_tokens:>3}t)")
                wavs, sr = tts.generate_voice_clone(
                    text=chunk,
                    language=language,
                    voice_clone_prompt=voice_clone_prompt,
                    **gen_kwargs,
                )
                segment_samples = len(wavs[0])
                samples_per_token = (
                    segment_samples / chunk_tokens if chunk_tokens > 0 else 0.0
                )
                # statistics
                sample_per_token_history.append(samples_per_token)
                running_avg = statistics.mean(sample_per_token_history)
                std_dev = statistics.stdev(sample_per_token_history)

                # Z-score: how many standard deviations away
                z_score = abs(samples_per_token - running_avg) / std_dev if std_dev > 0 else 0
                is_outlier = z_score > std_dev_limit
                flag = " !!!" if is_outlier else ""

                print(
                    f"    Chunk {i+1:>3}/{total_chunks:<3} | {len(chunk):>3}c | {chunk_tokens:>3}t | "
                    f"{segment_samples:>7,}s | {samples_per_token:>7,.2f} s/t | {segment_samples/sr:>5.2f}s | "
                    f"{running_avg:>7,.2f}avg | {std_dev:>6,.2f}Ïƒ | {z_score:>4.2f}z{flag}",
                    flush=True,
                )
                # Write raw float32 PCM samples directly to disk
                pcm_file.write(wavs[0].astype(np.float32).tobytes())
                sample_rate = sr
        
        if sample_rate is None:
            raise RuntimeError("No audio generated (empty text?)")
        
        # Convert raw PCM to WAV using ffmpeg (no need to load into memory)
        print(f"  Writing WAV from raw PCM...", flush=True)
        cmd = [
            "ffmpeg", "-y",
            "-f", "f32le",           # raw float32 little-endian
            "-ar", str(sample_rate), # sample rate
            "-ac", "1",              # mono
            "-i", str(pcm_path),
            "-c:a", "pcm_s16le",     # standard 16-bit WAV
            str(output_path),
        ]
        subprocess.run(cmd, capture_output=True, check=True)
    finally:
        # Clean up the raw PCM file
        if pcm_path.exists():
            pcm_path.unlink()


def convert_audio(wav_path: Path, output_path: Path, config: dict) -> int:
    """Convert WAV to final format. Returns duration in seconds."""
    audio_format = config["tts"].get("audio_format", "opus")

    if audio_format == "opus":
        bitrate = config["tts"].get("opus_bitrate", 48)
        cmd = [
            "ffmpeg", "-y", "-i", str(wav_path),
            "-c:a", "libopus", "-b:a", f"{bitrate}k",
            str(output_path),
        ]
    else:  # mp3
        bitrate = config["tts"].get("mp3_bitrate", 64)
        cmd = [
            "ffmpeg", "-y", "-i", str(wav_path),
            "-c:a", "libmp3lame", "-b:a", f"{bitrate}k",
            str(output_path),
        ]

    subprocess.run(cmd, capture_output=True, check=True)

    # Get duration using ffprobe
    probe_cmd = [
        "ffprobe", "-v", "error",
        "-show_entries", "format=duration",
        "-of", "default=noprint_wrappers=1:nokey=1",
        str(output_path),
    ]
    result = subprocess.run(probe_cmd, capture_output=True, text=True, check=True)
    duration = int(float(result.stdout.strip()))

    return duration


def process_job(job_path: Path, config: dict, paths: dict):
    """Process a single job file."""
    import time
    
    job_name = job_path.stem
    start_time = datetime.now(timezone.utc)
    start_mono = time.monotonic()
    
    print(f"\n{'='*60}")
    print(f"Processing job: {job_name}")
    print(f"Started: {start_time.isoformat()}")
    print(f"{'='*60}")
    
    write_status(paths, f"Processing: {job_name}")

    # Load job
    with open(job_path) as f:
        job = json.load(f)

    # Move to processing
    processing_path = paths["processing"] / job_path.name
    shutil.move(job_path, processing_path)

    try:
        # Extract text
        write_status(paths, f"Extracting text: {job_name}")
        text = extract_text(job, config)
        print(f"  Extracted {len(text)} characters")

        # Determine output format
        audio_format = config["tts"].get("audio_format", "opus")
        audio_ext = "opus" if audio_format == "opus" else "mp3"
        audio_filename = f"{job_name}.{audio_ext}"
        audio_output = paths["audio"] / audio_filename

        # Generate TTS audio
        write_status(paths, f"Generating audio: {job_name}")
        with tempfile.TemporaryDirectory() as tmpdir:
            wav_path = Path(tmpdir) / "output.wav"
            generate_audio(text, wav_path, job, config, paths)

            # Convert to final format
            write_status(paths, f"Converting audio: {job_name}")
            duration = convert_audio(wav_path, audio_output, config)

        # Add completion metadata to job
        end_time = datetime.now(timezone.utc)
        elapsed_seconds = round(time.monotonic() - start_mono, 2)
        
        job["_completed"] = {
            "started": start_time.isoformat(),
            "completed": end_time.isoformat(),
            "elapsed_seconds": elapsed_seconds,
            "audio_file": audio_filename,
            "audio_size": audio_output.stat().st_size,
            "duration_seconds": duration,
            "guid": job_name,
            "tts_model": TTS_MODEL_NAME,
        }
        
        print(f"\n{'='*60}")
        print(f"Job completed: {job_name}")
        print(f"  Elapsed: {elapsed_seconds}s")
        print(f"  Audio duration: {duration}s")
        print(f"  RTF: {elapsed_seconds / duration:.2f}x" if duration > 0 else "")
        print(f"{'='*60}")

        # Ensure title exists
        if "title" not in job:
            if job["type"] == "url":
                job["title"] = job["url"]
            else:
                job["title"] = "Untitled"

        # Move to done
        done_path = paths["done"] / job_path.name
        with open(done_path, "w") as f:
            json.dump(job, f, indent=2)
        processing_path.unlink()

        # Update RSS feed
        write_status(paths, f"Updating feed: {job_name}")
        update_feed_script = Path(__file__).parent / "update-feed"
        subprocess.run([sys.executable, str(update_feed_script)], check=True)

        write_status(paths, f"Completed: {job_name}")

    except Exception as e:
        # Move to failed state (back to done with error)
        job["_error"] = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "message": str(e),
        }
        failed_path = paths["done"] / f"FAILED-{job_path.name}"
        with open(failed_path, "w") as f:
            json.dump(job, f, indent=2)
        processing_path.unlink()
        write_status(paths, f"FAILED: {job_name} - {e}")
        raise


def main():
    """Main entry point."""
    # Load config
    config_path = os.environ.get("REEDER_CONFIG")
    config = load_config(Path(config_path) if config_path else None)
    paths = get_paths(config)

    # Ensure directories exist
    for name, path in paths.items():
        if name != "status":
            path.mkdir(parents=True, exist_ok=True)

    # Process all jobs in queue
    processed = 0
    while True:
        job_path = get_oldest_job(paths["inbox"])
        if not job_path:
            break
        
        try:
            process_job(job_path, config, paths)
            processed += 1
        except Exception as e:
            # Job failed but continue with next
            print(f"Job failed: {e}", file=sys.stderr)
            processed += 1

    if processed == 0:
        write_status(paths, "Idle - no jobs in queue")
    else:
        write_status(paths, f"Idle - processed {processed} job(s)")


if __name__ == "__main__":
    main()
