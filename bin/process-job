#!/usr/bin/env python3
"""Reeder Job Processor

Processes TTS jobs from the inbox directory:
1. Picks the oldest job file
2. Moves it to processing/
3. Extracts text (for URL jobs) or uses provided text
4. Generates audio with pocket-tts
5. Converts to final format (opus/mp3)
6. Updates RSS feed
7. Moves completed job to done/
"""

# Auto-bootstrap: re-exec with uv if dependencies missing
def _bootstrap():
    try:
        import trafilatura  # noqa: F401
    except ImportError:
        import os, sys
        # Re-exec with uv run using /usr/lib/reeder as project
        # UV_PROJECT_ENVIRONMENT should be set by systemd to /var/lib/reeder/.venv
        # Use --frozen to prevent writing to uv.lock in read-only /usr/lib
        project_root = "/usr/lib/reeder" if os.path.exists("/usr/lib/reeder/pyproject.toml") else os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        frozen = ["--frozen"] if project_root == "/usr/lib/reeder" else []
        os.execvp("uv", ["uv", "run", "--project", project_root] + frozen + [sys.argv[0]] + sys.argv[1:])
_bootstrap()

import json
import os
import shutil
import subprocess
import sys
import tempfile
import tomllib
from datetime import datetime, timezone
from pathlib import Path
from urllib.parse import urlparse

# Status file for remote monitoring
STATUS_FILE = None


def load_config(config_path: Path | None = None) -> dict:
    """Load configuration from config.toml."""
    if config_path is None:
        # Look for config in standard locations
        candidates = [
            Path(__file__).parent.parent / "config.toml",
            Path("/etc/reeder/config.toml"),
            Path.home() / ".config/reeder/config.toml",
        ]
        for candidate in candidates:
            if candidate.exists():
                config_path = candidate
                break
        else:
            raise FileNotFoundError("No config.toml found")

    with open(config_path, "rb") as f:
        return tomllib.load(f)


def get_paths(config: dict) -> dict[str, Path]:
    """Resolve all paths from config."""
    base = Path(config["paths"]["base_dir"])
    return {
        "base": base,
        "inbox": base / config["paths"]["inbox"],
        "processing": base / config["paths"]["processing"],
        "done": base / config["paths"]["done"],
        "audio": base / config["paths"]["audio"],
        "www": base / config["paths"]["www"],
        "voices": base / config["paths"]["voices"],
        "status": base / config["paths"]["status_file"],
    }


def write_status(paths: dict, message: str):
    """Write status message to status file."""
    status_path = paths["status"]
    status_path.parent.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now(timezone.utc).isoformat()
    with open(status_path, "w") as f:
        f.write(f"{timestamp}\n{message}\n")
    print(f"[STATUS] {message}")


def get_oldest_job(inbox: Path) -> Path | None:
    """Get the oldest job file from inbox (by filename, which includes timestamp)."""
    jobs = sorted(inbox.glob("*.json"))
    return jobs[0] if jobs else None


def extract_text_trafilatura(url: str) -> str:
    """Extract article text using trafilatura."""
    import trafilatura

    downloaded = trafilatura.fetch_url(url)
    if not downloaded:
        raise ValueError(f"Failed to fetch URL: {url}")

    text = trafilatura.extract(
        downloaded,
        include_comments=False,
        include_tables=False,
    )
    if not text:
        raise ValueError(f"Failed to extract text from: {url}")

    return text


def extract_text_pup(url: str, selector: str) -> str:
    """Extract text using curl + pup with a CSS selector."""
    result = subprocess.run(
        ["curl", "-sL", url],
        capture_output=True,
        text=True,
        check=True,
    )
    html = result.stdout

    result = subprocess.run(
        ["pup", selector],
        input=html,
        capture_output=True,
        text=True,
        check=True,
    )
    text = result.stdout.strip()

    if not text:
        raise ValueError(f"No text found with selector '{selector}' at {url}")

    return text


def extract_text_readability(url: str) -> str:
    """Extract text using readability-cli."""
    result = subprocess.run(
        ["readable", url, "-p"],
        capture_output=True,
        text=True,
        check=True,
    )
    text = result.stdout.strip()

    if not text:
        raise ValueError(f"Failed to extract text with readability from: {url}")

    return text


def extract_text(job: dict, config: dict) -> str:
    """Extract text based on job type and extractor settings."""
    if job["type"] == "text":
        return job["text"]

    url = job["url"]
    extractor = job.get("extractor", config["extractors"]["default"])

    # Check for site-specific extractor
    if extractor == "auto":
        domain = urlparse(url).netloc
        site_extractors = config.get("extractors", {}).get("sites", {})
        if domain in site_extractors:
            extractor = {"pup": site_extractors[domain]}
        else:
            extractor = "trafilatura"

    if isinstance(extractor, dict) and "pup" in extractor:
        return extract_text_pup(url, extractor["pup"])
    elif extractor == "readability":
        return extract_text_readability(url)
    else:  # trafilatura (default)
        return extract_text_trafilatura(url)


# Global TTS model (lazy-loaded to avoid startup cost when no jobs)
_tts_model = None
TTS_MODEL_NAME = "Qwen/Qwen3-TTS-12Hz-0.6B-Base"


def get_tts_model(device: str = "cpu"):
    """Get or load the Qwen3-TTS model."""
    global _tts_model
    if _tts_model is None:
        import torch
        from qwen_tts import Qwen3TTSModel
        
        print(f"Loading TTS model: {TTS_MODEL_NAME}")
        print(f"  Device: {device}")
        print(f"  Dtype: torch.float32")
        _tts_model = Qwen3TTSModel.from_pretrained(
            TTS_MODEL_NAME,
            device_map=device,
            dtype=torch.float32,
        )
        print(f"  Model loaded successfully.")
    return _tts_model


def generate_audio(text: str, output_path: Path, job: dict, config: dict, paths: dict):
    """Generate audio using Qwen3-TTS."""
    import soundfile as sf
    
    voice = job.get("voice", "default")
    
    # Resolve voice to .wav file
    if voice == "default":
        default_voice = config["tts"]["default_voice"]
        # Strip extension and add .wav
        voice_name = Path(default_voice).stem
        voice_wav = paths["voices"] / f"{voice_name}.wav"
    elif not Path(voice).is_absolute():
        voice_name = Path(voice).stem
        voice_wav = paths["voices"] / f"{voice_name}.wav"
    else:
        voice_wav = Path(voice)
        voice_name = voice_wav.stem
    
    # Find transcript file (.txt next to .wav)
    voice_txt = voice_wav.with_suffix(".txt")
    if not voice_txt.exists():
        raise ValueError(f"Voice transcript not found: {voice_txt}")
    if not voice_wav.exists():
        raise ValueError(f"Voice audio not found: {voice_wav}")
    
    ref_text = voice_txt.read_text().strip()
    
    device = config["tts"].get("device", "cpu")
    temperature = job.get("temperature", config["tts"].get("temperature", 0.9))
    language = job.get("language", "Auto")
    
    print(f"  Voice: {voice_name}")
    print(f"  Reference audio: {voice_wav}")
    print(f"  Temperature: {temperature}")
    print(f"  Language: {language}")
    
    # Load model
    tts = get_tts_model(device)
    
    # Generate audio
    gen_kwargs = dict(
        max_new_tokens=4096,
        do_sample=True,
        top_k=50,
        top_p=1.0,
        temperature=temperature,
        repetition_penalty=1.05,
        subtalker_dosample=True,
        subtalker_top_k=50,
        subtalker_top_p=1.0,
        subtalker_temperature=temperature,
    )
    
    wavs, sr = tts.generate_voice_clone(
        text=text,
        language=language,
        ref_audio=str(voice_wav),
        ref_text=ref_text,
        x_vector_only_mode=False,
        **gen_kwargs,
    )
    
    # Save output
    sf.write(str(output_path), wavs[0], sr)


def convert_audio(wav_path: Path, output_path: Path, config: dict) -> int:
    """Convert WAV to final format. Returns duration in seconds."""
    audio_format = config["tts"].get("audio_format", "opus")

    if audio_format == "opus":
        bitrate = config["tts"].get("opus_bitrate", 48)
        cmd = [
            "ffmpeg", "-y", "-i", str(wav_path),
            "-c:a", "libopus", "-b:a", f"{bitrate}k",
            str(output_path),
        ]
    else:  # mp3
        bitrate = config["tts"].get("mp3_bitrate", 64)
        cmd = [
            "ffmpeg", "-y", "-i", str(wav_path),
            "-c:a", "libmp3lame", "-b:a", f"{bitrate}k",
            str(output_path),
        ]

    subprocess.run(cmd, capture_output=True, check=True)

    # Get duration using ffprobe
    probe_cmd = [
        "ffprobe", "-v", "error",
        "-show_entries", "format=duration",
        "-of", "default=noprint_wrappers=1:nokey=1",
        str(output_path),
    ]
    result = subprocess.run(probe_cmd, capture_output=True, text=True, check=True)
    duration = int(float(result.stdout.strip()))

    return duration


def process_job(job_path: Path, config: dict, paths: dict):
    """Process a single job file."""
    import time
    
    job_name = job_path.stem
    start_time = datetime.now(timezone.utc)
    start_mono = time.monotonic()
    
    print(f"\n{'='*60}")
    print(f"Processing job: {job_name}")
    print(f"Started: {start_time.isoformat()}")
    print(f"{'='*60}")
    
    write_status(paths, f"Processing: {job_name}")

    # Load job
    with open(job_path) as f:
        job = json.load(f)

    # Move to processing
    processing_path = paths["processing"] / job_path.name
    shutil.move(job_path, processing_path)

    try:
        # Extract text
        write_status(paths, f"Extracting text: {job_name}")
        text = extract_text(job, config)
        print(f"  Extracted {len(text)} characters")

        # Determine output format
        audio_format = config["tts"].get("audio_format", "opus")
        audio_ext = "opus" if audio_format == "opus" else "mp3"
        audio_filename = f"{job_name}.{audio_ext}"
        audio_output = paths["audio"] / audio_filename

        # Generate TTS audio
        write_status(paths, f"Generating audio: {job_name}")
        with tempfile.TemporaryDirectory() as tmpdir:
            wav_path = Path(tmpdir) / "output.wav"
            generate_audio(text, wav_path, job, config, paths)

            # Convert to final format
            write_status(paths, f"Converting audio: {job_name}")
            duration = convert_audio(wav_path, audio_output, config)

        # Add completion metadata to job
        end_time = datetime.now(timezone.utc)
        elapsed_seconds = round(time.monotonic() - start_mono, 2)
        
        job["_completed"] = {
            "started": start_time.isoformat(),
            "completed": end_time.isoformat(),
            "elapsed_seconds": elapsed_seconds,
            "audio_file": audio_filename,
            "audio_size": audio_output.stat().st_size,
            "duration_seconds": duration,
            "guid": job_name,
            "tts_model": TTS_MODEL_NAME,
        }
        
        print(f"\n{'='*60}")
        print(f"Job completed: {job_name}")
        print(f"  Elapsed: {elapsed_seconds}s")
        print(f"  Audio duration: {duration}s")
        print(f"  RTF: {elapsed_seconds / duration:.2f}x" if duration > 0 else "")
        print(f"{'='*60}")

        # Ensure title exists
        if "title" not in job:
            if job["type"] == "url":
                job["title"] = job["url"]
            else:
                job["title"] = "Untitled"

        # Move to done
        done_path = paths["done"] / job_path.name
        with open(done_path, "w") as f:
            json.dump(job, f, indent=2)
        processing_path.unlink()

        # Update RSS feed
        write_status(paths, f"Updating feed: {job_name}")
        update_feed_script = Path(__file__).parent / "update-feed"
        subprocess.run([sys.executable, str(update_feed_script)], check=True)

        write_status(paths, f"Completed: {job_name}")

    except Exception as e:
        # Move to failed state (back to done with error)
        job["_error"] = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "message": str(e),
        }
        failed_path = paths["done"] / f"FAILED-{job_path.name}"
        with open(failed_path, "w") as f:
            json.dump(job, f, indent=2)
        processing_path.unlink()
        write_status(paths, f"FAILED: {job_name} - {e}")
        raise


def main():
    """Main entry point."""
    # Load config
    config_path = os.environ.get("REEDER_CONFIG")
    config = load_config(Path(config_path) if config_path else None)
    paths = get_paths(config)

    # Ensure directories exist
    for name, path in paths.items():
        if name != "status":
            path.mkdir(parents=True, exist_ok=True)

    # Process all jobs in queue
    processed = 0
    while True:
        job_path = get_oldest_job(paths["inbox"])
        if not job_path:
            break
        
        try:
            process_job(job_path, config, paths)
            processed += 1
        except Exception as e:
            # Job failed but continue with next
            print(f"Job failed: {e}", file=sys.stderr)
            processed += 1

    if processed == 0:
        write_status(paths, "Idle - no jobs in queue")
    else:
        write_status(paths, f"Idle - processed {processed} job(s)")


if __name__ == "__main__":
    main()
